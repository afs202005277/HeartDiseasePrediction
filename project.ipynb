{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Project 2 - Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let's first check the data in the **train.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T14:44:08.985813900Z",
     "start_time": "2023-05-21T14:44:07.048919200Z"
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import time\n",
    "\n",
    "train_data_imbalanced = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let's check if there are columns with missing values and change the non-numeric columns to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T11:42:18.040894400Z",
     "start_time": "2023-05-21T11:42:17.837664300Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values: []\n"
     ]
    }
   ],
   "source": [
    "cols_with_na = train_data_imbalanced.columns[train_data_imbalanced.isna().any()].tolist()\n",
    "print('Columns with missing values:', cols_with_na)\n",
    "\n",
    "train_data_imbalanced.drop('policy_id', axis=1, inplace=True)\n",
    "train_data_imbalanced['area_cluster'] = train_data_imbalanced['area_cluster'].apply(lambda x: x.replace('C', ''))\n",
    "train_data_imbalanced = pd.get_dummies(train_data_imbalanced,\n",
    "                            columns=['segment', 'fuel_type', 'engine_type', 'rear_brakes_type', 'steering_type'])\n",
    "train_data_imbalanced['model'] = train_data_imbalanced['model'].apply(lambda x: x.replace('M', ''))\n",
    "train_data_imbalanced = train_data_imbalanced.replace({\"Yes\": True, \"No\": False})\n",
    "encoder = LabelEncoder()\n",
    "train_data_imbalanced['transmission_type'] = encoder.fit_transform(train_data_imbalanced['transmission_type'])\n",
    "\n",
    "train_data_imbalanced['max_torque'] = train_data_imbalanced['max_torque'].apply(\n",
    "    lambda s: float(s.split('Nm@')[0]) * float(s.split('Nm@')[1].replace('rpm', '')))\n",
    "train_data_imbalanced['max_power'] = train_data_imbalanced['max_power'].apply(\n",
    "    lambda s: float(s.split('bhp@')[0]) * float(s.split('bhp@')[1].replace('rpm', '')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see, there are no missing values in our data set\n",
    "\n",
    "### Now lets deal with the sampling problem: our dataset contains a significant class imbalance problem in the target column (\"is_claim\"), with the majority of observations belonging to the 0 class and only a minority belonging to the 1 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T11:42:18.077858600Z",
     "start_time": "2023-05-21T11:42:18.025880600Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       policy_tenure  age_of_car  age_of_policyholder area_cluster   \n",
      "486         0.011484        0.00             0.442308            5  \\\n",
      "14103       0.225361        0.03             0.317308            7   \n",
      "44119       1.154077        0.04             0.471154           13   \n",
      "52162       1.153476        0.05             0.528846            8   \n",
      "34576       1.058384        0.22             0.548077            8   \n",
      "\n",
      "       population_density  make model  max_torque  max_power  airbags  ...   \n",
      "486                 34738     1     1    210000.0   242160.0        2  ...  \\\n",
      "14103                6112     1     1    210000.0   242160.0        2  ...   \n",
      "44119                5410     5     9    350000.0   352404.0        2  ...   \n",
      "52162                8794     3     4    687500.0   453800.0        6  ...   \n",
      "34576                8794     3     4    687500.0   453800.0        6  ...   \n",
      "\n",
      "       engine_type_F8D Petrol Engine  engine_type_G12B   \n",
      "486                             True             False  \\\n",
      "14103                           True             False   \n",
      "44119                          False             False   \n",
      "52162                          False             False   \n",
      "34576                          False             False   \n",
      "\n",
      "       engine_type_K Series Dual jet  engine_type_K10C  engine_type_i-DTEC   \n",
      "486                            False             False               False  \\\n",
      "14103                          False             False               False   \n",
      "44119                          False             False                True   \n",
      "52162                          False             False               False   \n",
      "34576                          False             False               False   \n",
      "\n",
      "       rear_brakes_type_Disc  rear_brakes_type_Drum  steering_type_Electric   \n",
      "486                    False                   True                   False  \\\n",
      "14103                  False                   True                   False   \n",
      "44119                  False                   True                    True   \n",
      "52162                   True                  False                   False   \n",
      "34576                   True                  False                   False   \n",
      "\n",
      "       steering_type_Manual  steering_type_Power  \n",
      "486                   False                 True  \n",
      "14103                 False                 True  \n",
      "44119                 False                False  \n",
      "52162                 False                 True  \n",
      "34576                 False                 True  \n",
      "\n",
      "[5 rows x 63 columns]\n",
      "7496\n"
     ]
    }
   ],
   "source": [
    "claimed = train_data_imbalanced[train_data_imbalanced['is_claim'] == 1]\n",
    "not_claimed = train_data_imbalanced[train_data_imbalanced['is_claim'] == 0]\n",
    "sample_size = min(len(claimed), len(not_claimed))\n",
    "train_data = pd.concat([claimed.sample(n=sample_size), not_claimed.sample(n=sample_size)])\n",
    "\n",
    "print(train_data.head())\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "We had to have the same number of claims and not claims to avoid **imbalance**\n",
    "\n",
    "### Let's now get some more general information of our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(train_data.describe())\n",
    "#sb.pairplot(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After analysing the output, we can verify that there are no major outliers that require work.\n",
    "We can also conclude that there are some variables that seem to have no correlation with the `is_claim` feature.\n",
    "\n",
    "### Now let's check what features have no correlation with the test feature\n",
    "\n",
    "We will also create some subsets with:\n",
    "  - Data without >= 0.9 correlated data\n",
    "  - Data without >= 0.8 correlated data\n",
    "  - Data without >= 0.7 correlated data\n",
    "  - Data correlated with the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numeric_columns = train_data.select_dtypes(include='number')\n",
    "train_data_numeric = train_data[numeric_columns.columns]\n",
    "\n",
    "\n",
    "correlation_matrix = train_data_numeric.corr()\n",
    "\n",
    "# Creating the subsets without the highly correlated features\n",
    "corr_cols08 = set()\n",
    "corr_cols09 = set()\n",
    "corr_cols07 = set()\n",
    "nocorr = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if correlation_matrix.columns[i] != correlation_matrix.columns[j] and abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[j]\n",
    "            corr_cols08.add(colname)\n",
    "\n",
    "        if correlation_matrix.columns[i] != correlation_matrix.columns[j] and abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            colname = correlation_matrix.columns[j]\n",
    "            corr_cols09.add(colname)\n",
    "\n",
    "        if correlation_matrix.columns[i] != correlation_matrix.columns[j] and abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            colname = correlation_matrix.columns[j]\n",
    "            corr_cols07.add(colname)\n",
    "    if correlation_matrix.columns[i] != \"is_claim\":\n",
    "        j = (len(correlation_matrix.columns) - 1)\n",
    "        if abs((correlation_matrix.iloc[i, j])) < 0.02:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            nocorr.add(colname)\n",
    "\n",
    "train_data_08 = train_data_numeric.drop(columns=corr_cols08)\n",
    "train_data_09 = train_data_numeric.drop(columns=corr_cols09)\n",
    "train_data_07 = train_data_numeric.drop(columns=corr_cols07)\n",
    "nocorr.discard('age_of_policyholder')\n",
    "train_data_nocorr = train_data_numeric.drop(columns=nocorr)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "sb.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To test the relation between the features we created a new feature that contains a combination of other ones:\n",
    "\n",
    "- Create a new column `combination` that uses the most important features to the target\n",
    "- Use some preprocessing tools (Polynomial and Scaler) to create some more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Combination feature\n",
    "\n",
    "train_data_nocorr.insert(len(train_data_nocorr.columns) - 1, 'combination', train_data_nocorr['policy_tenure'] * train_data_nocorr['age_of_policyholder'] - train_data_nocorr['age_of_car'])\n",
    "\n",
    "correlation_matrix = train_data_nocorr.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "sb.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Polynomial\n",
    "\n",
    "target = \"is_claim\"\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X = train_data_nocorr.drop(target, axis=1)\n",
    "y = train_data_nocorr[target]\n",
    "\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "train_data_poly = pd.concat([pd.DataFrame(X_poly), y], axis=1).dropna()\n",
    "\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "train_data_scaled = pd.concat([pd.DataFrame(X_scaled), y], axis=1).dropna()\n",
    "\n",
    "# Polynomial and Scale\n",
    "\n",
    "X_poly_scaled = poly.fit_transform(X_scaled)\n",
    "\n",
    "train_data_poly_scaled = pd.concat([pd.DataFrame(X_poly_scaled), y], axis=1).dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Now we are going to check if there are any nonnumerical features that have a high impact on the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "non_numeric_columns = train_data.select_dtypes(exclude='number')\n",
    "for column in non_numeric_columns:\n",
    "    if 'id' in column: continue\n",
    "    crosstab_table = pd.crosstab(train_data[column], train_data['is_claim'])\n",
    "    crosstab_table.plot(kind='bar', stacked=True)\n",
    "\n",
    "    plt.title(f'Crosstab for {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Numeric features don't correlate significantly with the `is_claim` feature, but there are others features that strongly correlate with each other, which might give more strength to those values, something that we want to avoid.\n",
    "A good way of avoiding it is to remove the column that correlate too much with another. For this assignment, we tested removing columns with correlation greater than 0.70, 0.80 and 0.90.\n",
    "\n",
    "In non-numeric features we didn't get a clear separation but there are some choices that are more favorable. For example in the feature `segment`, people insure more an `A` than a `Utility` value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let's now dive into the classifier methods\n",
    "\n",
    "We have done an initial comparison of all the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Neural Network': MLPClassifier(),\n",
    "    'K-NN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'SGD': SGDClassifier()\n",
    "}\n",
    "target_column = \"is_claim\"\n",
    "X = train_data_imbalanced.drop(target_column, axis=1)\n",
    "y = train_data_imbalanced[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train and evaluate the classifiers\n",
    "results = []\n",
    "\n",
    "for (name, classifier) in classifiers.items():\n",
    "\n",
    "    for _ in range(5):\n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        # Test the model\n",
    "        start_time = time.time()\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        test_time = time.time() - start_time\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'Classifier': name,\n",
    "            'Confusion Matrix': conf_matrix,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1,\n",
    "            'Train Time': train_time,\n",
    "            'Test Time': test_time\n",
    "        })\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "sb.barplot(x='Classifier', y='Accuracy', data=results_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see in the confusion matrix, we are having very high accuracies because our model is only predicting `No` (Some classifiers rarely predict `Yes`). This is the reason we **undersampled**\n",
    "\n",
    "### Neural Network\n",
    "\n",
    "Until now, we have only used the default data and model settings.\n",
    "We are now going to tweak those settings and values to try to get the best results possible.\n",
    "\n",
    "We tested 8 sets:\n",
    "\n",
    "- Default Set\n",
    "- Set without more than 90% correlated information\n",
    "- Set without more than 80% correlated information\n",
    "- Set without more than 70% correlated information\n",
    "- Only data that is correlated to the target feature\n",
    "- Polynomial feature data\n",
    "- Scaled feature data\n",
    "- Polynomial scaled feature data\n",
    "\n",
    "With 3 division methods:\n",
    "\n",
    "- Random division (70/30)\n",
    "- Random division (80/20)\n",
    "- Stratified K Folds (3 folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sets = {\n",
    "    'Default Set': train_data,\n",
    "    '>= 0.9 correlation': train_data_09,\n",
    "    '>= 0.8 correlation': train_data_08,\n",
    "    '>= 0.7 correlation': train_data_07,\n",
    "    'Only correlated': train_data_nocorr,\n",
    "    'Polynomial': train_data_poly,\n",
    "    'Scaled': train_data_scaled,\n",
    "    'Poly Scaled': train_data_poly_scaled\n",
    "}\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for (name, data_set) in sets.items():\n",
    "    # Train the model\n",
    "    for _ in range(5):  # running 5 times each\n",
    "        neural_network = MLPClassifier(hidden_layer_sizes=(10, 20, 10), activation='logistic', solver='adam', alpha=0.0001, learning_rate_init=0.002)\n",
    "        target_column = \"is_claim\"\n",
    "        X = data_set.drop(target_column, axis=1)\n",
    "        y = data_set[target_column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        start_time = time.time()\n",
    "\n",
    "        neural_network.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        # Test the model\n",
    "        start_time = time.time()\n",
    "        y_pred = neural_network.predict(X_test)\n",
    "        test_time = time.time() - start_time\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'Set': name,\n",
    "            'Confusion Matrix': conf_matrix,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1,\n",
    "            'Train Time': train_time,\n",
    "            'Test Time': test_time\n",
    "        })\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Neural Network - Random Division (70/30)\")\n",
    "sb.barplot(x='Set', y='Accuracy', data=results_df)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Neural Network - Random Division (70/30)\")\n",
    "sb.barplot(x='Set', y='F1 Score', data=results_df)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Neural Network - Random Division (70/30)\")\n",
    "sb.barplot(x='Set', y='Precision', data=results_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for (name, data_set) in sets.items():\n",
    "    # Train the model\n",
    "    for _ in range(5):  # running 5 times each\n",
    "        neural_network = MLPClassifier(hidden_layer_sizes=(10, 20, 10), activation='logistic', solver='adam', alpha=0.0001, learning_rate_init=0.002)\n",
    "        target_column = \"is_claim\"\n",
    "        X = data_set.drop(target_column, axis=1)\n",
    "        y = data_set[target_column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        start_time = time.time()\n",
    "\n",
    "        neural_network.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        # Test the model\n",
    "        start_time = time.time()\n",
    "        y_pred = neural_network.predict(X_test)\n",
    "        test_time = time.time() - start_time\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'Set': name,\n",
    "            'Confusion Matrix': conf_matrix,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1,\n",
    "            'Train Time': train_time,\n",
    "            'Test Time': test_time\n",
    "        })\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Neural Network - Random Division (80/20)\")\n",
    "sb.barplot(x='Set', y='Accuracy', data=results_df)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Neural Network - Random Division (80/20)\")\n",
    "sb.barplot(x='Set', y='F1 Score', data=results_df)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Neural Network - Random Division (80/20)\")\n",
    "sb.barplot(x='Set', y='Precision', data=results_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "for (name, data_set) in sets.items():\n",
    "    # Train the model\n",
    "    neural_network = MLPClassifier(hidden_layer_sizes=(10, 20, 10), activation='logistic', solver='adam', alpha=0.0001, learning_rate_init=0.002)\n",
    "    target_column = \"is_claim\"\n",
    "    X = data_set.drop(target_column, axis=1)\n",
    "    y = data_set[target_column]\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.values[train_index], X.values[test_index]\n",
    "        y_train, y_test = y.values[train_index], y.values[test_index]\n",
    "\n",
    "        start_time = time.time()\n",
    "        neural_network.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        # Test the model\n",
    "        start_time = time.time()\n",
    "        y_pred = neural_network.predict(X_test)\n",
    "        test_time = time.time() - start_time\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'Set': name,\n",
    "            'Confusion Matrix': conf_matrix,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1,\n",
    "            'Train Time': train_time,\n",
    "            'Test Time': test_time\n",
    "        })\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Neural Network - Stratified 3 Folds\")\n",
    "sb.barplot(x='Set', y='Accuracy', data=results_df)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Neural Network - Stratified 3 Folds\")\n",
    "sb.barplot(x='Set', y='F1 Score', data=results_df)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Neural Network - Stratified 3 Folds\")\n",
    "sb.barplot(x='Set', y='Precision', data=results_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can conclude that the results didn't change much with the different sets and tweaks made in the neural network. Still, the `Only correlated` set performs well in the 80/20 random division (highest precision).\n",
    "\n",
    "In the `Stratified 3 Fold` some sets are predicting only `Yes` and other ones only `No` depending on how the fold division is made.\n",
    "\n",
    "There is still some possible work in the data to improve the overall results.\n",
    "\n",
    "### Now we will use the GridSearchCV component of the sklearn package in order to find out the parameters combination that produces the best outcome for every classifier\n",
    "\n",
    "### Furthermore, we also use some plots to make the comparison easier and more intuitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "names = ['Train Data', 'Train Data 07', 'Train Data 08', 'Train Data 09', 'Only Correlated', 'Polynomial', 'Scaled', 'Poly Scaled']\n",
    "datasets = [train_data, train_data_07, train_data_08, train_data_09, train_data_nocorr, train_data_poly, train_data_scaled, train_data_poly_scaled]\n",
    "target_column = \"is_claim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def grid_search_analysis(target, parameters, model, search_datasets, datasets_names):\n",
    "    search_results = []\n",
    "    for dataset in search_datasets:\n",
    "        X = dataset.drop(target, axis=1)\n",
    "        y = dataset[target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        grid_search = GridSearchCV(model, param_grid=parameters, cv=5, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_params = grid_search.best_params_\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        print(best_params)\n",
    "\n",
    "        conf_matrix_metrics = confusion_matrix(y_test, y_pred)\n",
    "        precision_metrics = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "        recall_metrics = recall_score(y_test, y_pred, average='weighted')\n",
    "        accuracy_metrics = accuracy_score(y_test, y_pred)\n",
    "        f1_metrics = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Store the search_results\n",
    "        search_results.append({\n",
    "            'Confusion Matrix': conf_matrix_metrics,\n",
    "            'Precision': precision_metrics,\n",
    "            'Recall': recall_metrics,\n",
    "            'Accuracy': accuracy_metrics,\n",
    "            'F1 Score': f1_metrics,\n",
    "        })\n",
    "    for i, result in enumerate(search_results):\n",
    "        # Create a heatmap of the confusion matrix\n",
    "        sb.heatmap(result['Confusion Matrix'], annot=True, cmap='Blues')\n",
    "\n",
    "        # Add labels and title to the plot\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.ylabel('True label')\n",
    "        plt.title(f'Confusion Matrix for ' + datasets_names[i])\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    metrics = ['Precision', 'Recall', 'Accuracy', 'F1 Score']\n",
    "    for metric in metrics:\n",
    "        # Extract the metric values for each dataset\n",
    "        values = [result[metric] for result in search_results]\n",
    "\n",
    "        # Add labels and title to the plot\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xlabel('Dataset')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'{metric} comparison across search_datasets')\n",
    "        # Create a bar plot to compare the metric values\n",
    "        sb.barplot(x=datasets_names, y=values)\n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Neural Network\n",
    "\n",
    "Regarding the Neural Network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'hidden_layer_sizes': [(100,), (50, 100), (50, 50, 100), (100, 100)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [200, 400, 600],\n",
    "    'early_stopping': [True, False]\n",
    "}\n",
    "\n",
    "grid_search_analysis(target_column, params, MLPClassifier(), datasets, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The results indicate that the Neural Network model performed the worst overall. Though the accuracies were similar across the datasets, the precision was higher for `Only Correlated` and `Polinomial` because they only predicted one option [`Yes`, `No`].\n",
    "\n",
    "The optimal parameters differed greatly. One thing we can note is that we would expect the activation function to be `logistic` or `relu` for a classification problem since they are the most common. However, some models preferred `tanh` activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Decision Tree\n",
    "\n",
    "Regarding the Decision Tree model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [5, 10, 20, 80, 160, 320, 640, None],\n",
    "    'min_samples_split': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'min_samples_leaf': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'max_features': [None] + [x for x in range(1, len(train_data.columns))]\n",
    "}\n",
    "grid_search_analysis(target_column, params, DecisionTreeClassifier(), datasets, names)\n",
    "# Best params results:\n",
    "# {'criterion': 'entropy', 'max_depth': 20, 'max_features': 32, 'min_samples_leaf': 0.1, 'min_samples_split': 0.5, 'splitter': 'best'}\n",
    "# {'criterion': 'entropy', 'max_depth': 20, 'max_features': 3, 'min_samples_leaf': 0.1, 'min_samples_split': 0.6, 'splitter': 'best'}\n",
    "# {'criterion': 'entropy', 'max_depth': 640, 'max_features': 5, 'min_samples_leaf': 0.1, 'min_samples_split': 0.6, 'splitter': 'best'}\n",
    "# {'criterion': 'gini', 'max_depth': 10, 'max_features': 6, 'min_samples_leaf': 0.1, 'min_samples_split': 0.2, 'splitter': 'best'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Based on the results obtained, the best set is the `Only Correlated`. It performs better in every graph.\n",
    "\n",
    "Looking at the `criterion` parameter, the dataset could select between `Gini` and `Entropy`.\n",
    "This selection would depend on the amount of impurities that are present in the data. For example, the `Only correlated` set should select the `Entropy` criterion, which is more concerned with the amount of information gained from a split, and it happens as expected.\n",
    "\n",
    "The `Train Data 07` should select the `Gini` critetion, which is appropriate given the presence of numerous irrelevant features, happening again as expected.\n",
    "\n",
    "The remaining parameters are primarily influenced by the size of the data sets. The default set, which is larger, requires more search and hence more computational resources. In contrast, the smaller 70% data set does not require as much computational effort as it has fewer data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "Regarding the K-Nearest Neighbors model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50],\n",
    "    'p': [1, 2],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev']\n",
    "}\n",
    "\n",
    "grid_search_analysis(target_column, params, KNeighborsClassifier(), datasets, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The results are largely consistent across data sets (the `Polynomial`, `Scaled` and `Poly Scaled` datasets perform the worst).\n",
    "\n",
    "Looking at the selected parameters:\n",
    "- The algorithm is typically set to `auto` since it chooses the optimal algorithm. However, `kd_tree` may be the best choice here given the moderate data size (6000 rows).\n",
    "- The `distance` weight is preferred for all data sets, indicating the data is not **evenly distributed**.\n",
    "- The metric depends on the data distribution but is primarily `manhattan`, suggesting the values have distances which they want to emphasize more than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "Regarding the Support Vector Machine model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [2, 3, 4, 5],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'coef0': [0, 1, 2, 3],\n",
    "    'shrinking': [True, False],\n",
    "    'tol': [1e-3, 1e-4, 1e-5],\n",
    "    'max_iter': [1000, 2000, 25000],\n",
    "    'decision_function_shape': ['ovr', 'ovo']\n",
    "}\n",
    "\n",
    "grid_search_analysis(target_column, params, SVC(), [train_data_scaled, train_data_poly_scaled], ['Scaled', 'Poly Scaled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Because of the complexity of the SVM algorithm, we could only test in the scaled datasets.\n",
    "\n",
    "They performed equally, being the `Poly Scaled` a little better.\n",
    "\n",
    "Looking at the parameters:\n",
    "\n",
    "- the `decision_function_shape` parameter selected is `ovr` which is a good choice when we have many features (62 columns)\n",
    "- the `kernel` parameter should be `linear` if the data can be linearly separable. As shown in the results, our data is not probably linearly separable because our sets are selecting `poly` and `sigmoid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Regarding the Stochastic Gradient Descent model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 2000, 3000],\n",
    "    'tol': [1e-3, 1e-4, 1e-5]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_analysis(target_column, params, SGDClassifier(), datasets, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "SGD shows very similar results when it comes to accuracy. Again, some datasets only selected one option in the target feature leading to high precision.\n",
    "\n",
    "From other tests, we concluded that our data is not linearly separable, which is something we can confirm looking at the `loss` parameter. It mostly chooses the `log` option, which is more suitable to use when the data is non-linearly separable data.\n",
    "\n",
    "Looking to the `penalty` parameter, our data sets normally prefer the `l1` option, which is an option that penalizes absolute weights (better for feature selection). `l2` is another good option, which penalizes the sum of the squared weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Conclusion:\n",
    "In conclusion, the decision tree model achieved the best results, with over 60% accuracy on the Only Correlated dataset.\n",
    "We could still improve the results with better data engineering and incorporating more data samples (i.e. more records of insured vehicles).\n",
    "This project provided valuable insights into the various classification techniques and helped clarify the differences between some machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}