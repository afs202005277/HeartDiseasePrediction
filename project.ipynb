{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Project 2 - Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let's first check the data in the **train.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:06:38.508081300Z",
     "start_time": "2023-05-20T19:06:36.301064600Z"
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import time\n",
    "\n",
    "train_data_imbalanced = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let's check if there are columns with missing values and change the non-numeric columns to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:06:40.601361200Z",
     "start_time": "2023-05-20T19:06:40.248038300Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols_with_na = train_data_imbalanced.columns[train_data_imbalanced.isna().any()].tolist()\n",
    "print('Columns with missing values:', cols_with_na)\n",
    "\n",
    "train_data_imbalanced.drop('policy_id', axis=1, inplace=True)\n",
    "train_data_imbalanced['area_cluster'] = train_data_imbalanced['area_cluster'].apply(lambda x: x.replace('C', ''))\n",
    "train_data_imbalanced = pd.get_dummies(train_data_imbalanced,\n",
    "                            columns=['segment', 'fuel_type', 'engine_type', 'rear_brakes_type', 'steering_type'])\n",
    "train_data_imbalanced['model'] = train_data_imbalanced['model'].apply(lambda x: x.replace('M', ''))\n",
    "train_data_imbalanced = train_data_imbalanced.replace({\"Yes\": True, \"No\": False})\n",
    "encoder = LabelEncoder()\n",
    "train_data_imbalanced['transmission_type'] = encoder.fit_transform(train_data_imbalanced['transmission_type'])\n",
    "\n",
    "train_data_imbalanced['max_torque'] = train_data_imbalanced['max_torque'].apply(\n",
    "    lambda s: float(s.split('Nm@')[0]) * float(s.split('Nm@')[1].replace('rpm', '')))\n",
    "train_data_imbalanced['max_power'] = train_data_imbalanced['max_power'].apply(\n",
    "    lambda s: float(s.split('bhp@')[0]) * float(s.split('bhp@')[1].replace('rpm', '')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see, there are no missing values in our data set\n",
    "\n",
    "### Now lets deal with the sampling problem: our dataset contains a significant class imbalance problem in the target column (\"is_claim\"), with the majority of observations belonging to the 0 class and only a minority belonging to the 1 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:06:48.036181900Z",
     "start_time": "2023-05-20T19:06:47.986543Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "claimed = train_data_imbalanced[train_data_imbalanced['is_claim'] == 1]\n",
    "not_claimed = train_data_imbalanced[train_data_imbalanced['is_claim'] == 0]\n",
    "sample_size = min(len(claimed), len(not_claimed))\n",
    "train_data = pd.concat([claimed.sample(n=sample_size), not_claimed.sample(n=sample_size)])\n",
    "\n",
    "print(train_data.head())\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "We had to have the same number of claims and not claims to avoid **imbalance**\n",
    "\n",
    "### Let's now get some more general information of our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T17:09:08.876181400Z",
     "start_time": "2023-05-18T17:06:54.674087600Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print(train_data.describe())\n",
    "sb.pairplot(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After analysing the output, we can verify that there are no major outliers that require work.\n",
    "We can also conclude that there are some variables that seem to have no correlation with the `is_claim` feature.\n",
    "\n",
    "### Now let's check what features have no correlation with the test feature\n",
    "\n",
    "We will also create some subsets with:\n",
    "  - Data without >= 0.9 correlated data\n",
    "  - Data without >= 0.8 correlated data\n",
    "  - Data without >= 0.7 correlated data\n",
    "  - Data correlated with the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:07:00.368611600Z",
     "start_time": "2023-05-20T19:06:58.842012400Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numeric_columns = train_data.select_dtypes(include='number')\n",
    "train_data_numeric = train_data[numeric_columns.columns]\n",
    "\n",
    "\n",
    "correlation_matrix = train_data_numeric.corr()\n",
    "\n",
    "# Creating the subsets without the highly correlated features\n",
    "corr_cols08 = set()\n",
    "corr_cols09 = set()\n",
    "corr_cols07 = set()\n",
    "nocorr = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if correlation_matrix.columns[i] != correlation_matrix.columns[j] and abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[j]\n",
    "            corr_cols08.add(colname)\n",
    "\n",
    "        if correlation_matrix.columns[i] != correlation_matrix.columns[j] and abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            colname = correlation_matrix.columns[j]\n",
    "            corr_cols09.add(colname)\n",
    "\n",
    "        if correlation_matrix.columns[i] != correlation_matrix.columns[j] and abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            colname = correlation_matrix.columns[j]\n",
    "            corr_cols07.add(colname)\n",
    "    if correlation_matrix.columns[i] != \"is_claim\":\n",
    "        j = (len(correlation_matrix.columns) - 1)\n",
    "        if abs((correlation_matrix.iloc[i, j])) < 0.02:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            nocorr.add(colname)\n",
    "\n",
    "train_data_08 = train_data_numeric.drop(columns=corr_cols08)\n",
    "train_data_09 = train_data_numeric.drop(columns=corr_cols09)\n",
    "train_data_07 = train_data_numeric.drop(columns=corr_cols07)\n",
    "nocorr.discard('age_of_policyholder')\n",
    "train_data_nocorr = train_data_numeric.drop(columns=nocorr)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "sb.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To test the relation between the features we created a new feature that contains a combination of other ones:\n",
    "\n",
    "- Create a new column `combination` that uses the most important features to the target\n",
    "- Use some preprocessing tools (Polynomial and Scaler) to create some more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:07:01.342985700Z",
     "start_time": "2023-05-20T19:07:00.855177500Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Combination feature\n",
    "\n",
    "train_data_nocorr.insert(len(train_data_nocorr.columns) - 1, 'combination', train_data_nocorr['policy_tenure'] * train_data_nocorr['age_of_policyholder'] - train_data_nocorr['age_of_car'])\n",
    "\n",
    "correlation_matrix = train_data_nocorr.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "sb.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Polynomial\n",
    "\n",
    "target = \"is_claim\"\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X = train_data_nocorr.drop(target, axis=1)\n",
    "y = train_data_nocorr[target]\n",
    "\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "train_data_poly = pd.concat([pd.DataFrame(X_poly), y], axis=1).dropna()\n",
    "\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "train_data_scaled = pd.concat([pd.DataFrame(X_scaled), y], axis=1).dropna()\n",
    "\n",
    "# Polynomial and Scale\n",
    "\n",
    "X_poly_scaled = poly.fit_transform(X_scaled)\n",
    "\n",
    "train_data_poly_scaled = pd.concat([pd.DataFrame(X_poly_scaled), y], axis=1).dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Now we are going to check if there are any nonnumerical features that have a high impact on the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T17:13:36.732897200Z",
     "start_time": "2023-05-18T17:13:23.978062400Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "non_numeric_columns = train_data.select_dtypes(exclude='number')\n",
    "\n",
    "for column in non_numeric_columns:\n",
    "    if 'id' in column: continue\n",
    "    crosstab_table = pd.crosstab(train_data[column], train_data['is_claim'])\n",
    "    crosstab_table.plot(kind='bar', stacked=True)\n",
    "\n",
    "    plt.title(f'Crosstab for {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Numeric features don't correlate significantly with the `is_claim` feature, but there are others features that strongly correlate with each other, which might give more strength to those values, something that we want to avoid.\n",
    "A good way of avoiding it is to remove the column that correlate too much with another. For this assignment, we tested removing columns with correlation greater than 0.70, 0.80 and 0.90.\n",
    "\n",
    "In non-numeric features we didn't get a clear separation but there are some choices that are more favorable. For example in the feature `segment`, people insure more an `A` than a `Utility` value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let's now dive into the classifier methods\n",
    "\n",
    "We have done an initial comparison of all the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:07:09.371262600Z",
     "start_time": "2023-05-20T19:07:09.332162500Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Neural Network': MLPClassifier(),\n",
    "    'K-NN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'SGD': SGDClassifier()\n",
    "}\n",
    "target_column = \"is_claim\"\n",
    "X = train_data.drop(target_column, axis=1)\n",
    "y = train_data[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:07:28.647493200Z",
     "start_time": "2023-05-20T19:07:10.338113Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train and evaluate the classifiers\n",
    "results = []\n",
    "\n",
    "for (name, classifier) in classifiers.items():\n",
    "\n",
    "    for _ in range(5):\n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        # Test the model\n",
    "        start_time = time.time()\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        test_time = time.time() - start_time\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'Classifier': name,\n",
    "            'Confusion Matrix': conf_matrix,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1,\n",
    "            'Train Time': train_time,\n",
    "            'Test Time': test_time\n",
    "        })\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "sb.barplot(x='Classifier', y='Accuracy', data=results_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see in the confusion matrix, we are having very high accuracies because our model is only predicting `No`. This is the reason we **undersampled**\n",
    "\n",
    "### Neural Network\n",
    "\n",
    "Until now, we have only used the default data and model settings.\n",
    "We are now going to tweak those settings and values to try to get the best results possible.\n",
    "\n",
    "We tested 8 sets:\n",
    "\n",
    "- Default Set\n",
    "- Set without more than 90% correlated information\n",
    "- Set without more than 80% correlated information\n",
    "- Set without more than 70% correlated information\n",
    "- Only data that is correlated to the target feature\n",
    "- Polynomial feature data\n",
    "- Scaled feature data\n",
    "- Polynomial scaled feature data\n",
    "\n",
    "With 3 division methods:\n",
    "\n",
    "- Random division (70/30)\n",
    "- Random division (80/20)\n",
    "- Stratified K Folds (3 folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:10:50.250325600Z",
     "start_time": "2023-05-20T19:10:43.201027200Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sets = {\n",
    "    'Default Set': train_data,\n",
    "    '>= 0.9 correlation': train_data_09,\n",
    "    '>= 0.8 correlation': train_data_08,\n",
    "    '>= 0.7 correlation': train_data_07,\n",
    "    'Only correlated': train_data_nocorr,\n",
    "    'Polynomial': train_data_poly,\n",
    "    'Scaled': train_data_scaled,\n",
    "    'Poly Scaled': train_data_poly_scaled\n",
    "}\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for (name, data_set) in sets.items():\n",
    "    # Train the model\n",
    "    for _ in range(5):  # running 5 times each\n",
    "        neural_network = MLPClassifier(hidden_layer_sizes=(10, 20, 10), activation='logistic', solver='adam', alpha=0.0001, learning_rate_init=0.002)\n",
    "        target_column = \"is_claim\"\n",
    "        X = data_set.drop(target_column, axis=1)\n",
    "        y = data_set[target_column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        start_time = time.time()\n",
    "\n",
    "        neural_network.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        # Test the model\n",
    "        start_time = time.time()\n",
    "        y_pred = neural_network.predict(X_test)\n",
    "        test_time = time.time() - start_time\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'Set': name,\n",
    "            'Confusion Matrix': conf_matrix,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1,\n",
    "            'Train Time': train_time,\n",
    "            'Test Time': test_time\n",
    "        })\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Neural Network - Random Division (70/30)\")\n",
    "sb.barplot(x='Set', y='Accuracy', data=results_df)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Neural Network - Random Division (70/30)\")\n",
    "sb.barplot(x='Set', y='F1 Score', data=results_df)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Neural Network - Random Division (70/30)\")\n",
    "sb.barplot(x='Set', y='Precision', data=results_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can conclude that the results didn't change much with the different sets and tweaks made in the neural network. Still, the `Only correlated` set performs well in the 80/20 random division (highest precision).\n",
    "\n",
    "There is still some possible work in the data to improve the overall results.\n",
    "\n",
    "### Now we will use the GridSearchCV component of the sklearn package in order to find out the parameters combination that produces the best outcome for every classifier\n",
    "\n",
    "### Furthermore, we also use some plots to make the comparison easier and more intuitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:11:30.763578300Z",
     "start_time": "2023-05-20T19:11:30.742758700Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "names = sets.keys()\n",
    "datasets = [sets[x] for x in names]\n",
    "target_column = \"is_claim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:11:33.397763900Z",
     "start_time": "2023-05-20T19:11:33.376745Z"
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def grid_search_analysis(target, parameters, model, search_datasets, datasets_names):\n",
    "    search_results = []\n",
    "    for dataset in search_datasets:\n",
    "        X = dataset.drop(target, axis=1)\n",
    "        y = dataset[target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        grid_search = GridSearchCV(model, param_grid=parameters, cv=5, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_params = grid_search.best_params_\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        print(best_params)\n",
    "\n",
    "        conf_matrix_metrics = confusion_matrix(y_test, y_pred)\n",
    "        precision_metrics = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "        recall_metrics = recall_score(y_test, y_pred, average='weighted')\n",
    "        accuracy_metrics = accuracy_score(y_test, y_pred)\n",
    "        f1_metrics = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Store the search_results\n",
    "        search_results.append({\n",
    "            'Confusion Matrix': conf_matrix_metrics,\n",
    "            'Precision': precision_metrics,\n",
    "            'Recall': recall_metrics,\n",
    "            'Accuracy': accuracy_metrics,\n",
    "            'F1 Score': f1_metrics,\n",
    "        })\n",
    "    for i, result in enumerate(search_results):\n",
    "        # Create a heatmap of the confusion matrix\n",
    "        sb.heatmap(result['Confusion Matrix'], annot=True, cmap='Blues')\n",
    "\n",
    "        # Add labels and title to the plot\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.ylabel('True label')\n",
    "        plt.title(f'Confusion Matrix for ' + datasets_names[i])\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    metrics = ['Precision', 'Recall', 'Accuracy', 'F1 Score']\n",
    "    for metric in metrics:\n",
    "        # Extract the metric values for each dataset\n",
    "        values = [result[metric] for result in search_results]\n",
    "\n",
    "        # Create a bar plot to compare the metric values\n",
    "        sb.barplot(x=datasets_names, y=values)\n",
    "\n",
    "        # Add labels and title to the plot\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xlabel('Dataset')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'{metric} comparison across search_datasets')\n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can conclude that the results didn't change much with the different sets and tweaks made in the neural network. Still, the `Only correlated` set performs well in the 80/20 random division (highest precision).\n",
    "\n",
    "There is still some possible work in the data to improve the overall results.\n",
    "\n",
    "### Now we will use the GridSearchCV component of the sklearn package in order to find out the parameters combination that produces the best outcome for every classifier\n",
    "\n",
    "### Furthermore, we also use some plots to make the comparison easier and more intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Neural Network\n",
    "\n",
    "Regarding the Neural Network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'hidden_layer_sizes': [(100,), (50, 100), (50, 50, 100), (100, 100)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [200, 400, 600],\n",
    "    'early_stopping': [True, False]\n",
    "}\n",
    "\n",
    "grid_search_analysis(target_column, params, MLPClassifier(), datasets, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that the Neural Network model performed the worst overall. Though the accuracies were similar across the datasets, the precision was higher for `Only Correlated` and `Train Data 09`.\n",
    "\n",
    "The optimal parameters differed greatly. One thing we can note is that we would expect the activation function to be `logistic` or `relu` for a classification problem since they are the most common. However, some models preferred `tanh` activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Decision Tree\n",
    "\n",
    "Regarding the Decision Tree model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:12:56.197712Z",
     "start_time": "2023-05-20T19:11:41.700031900Z"
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [1, 2, 5, 10, 20, 40, 80, 160, 320, 640, None],\n",
    "    'min_samples_split': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'min_samples_leaf': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'max_features': [None] + [x for x in range(1, len(train_data.columns))]\n",
    "}\n",
    "grid_search_analysis(target_column, params, DecisionTreeClassifier(), datasets, names)\n",
    "# Best params results:\n",
    "# {'criterion': 'entropy', 'max_depth': 20, 'max_features': 32, 'min_samples_leaf': 0.1, 'min_samples_split': 0.5, 'splitter': 'best'}\n",
    "# {'criterion': 'entropy', 'max_depth': 20, 'max_features': 3, 'min_samples_leaf': 0.1, 'min_samples_split': 0.6, 'splitter': 'best'}\n",
    "# {'criterion': 'entropy', 'max_depth': 640, 'max_features': 5, 'min_samples_leaf': 0.1, 'min_samples_split': 0.6, 'splitter': 'best'}\n",
    "# {'criterion': 'gini', 'max_depth': 10, 'max_features': 6, 'min_samples_leaf': 0.1, 'min_samples_split': 0.2, 'splitter': 'best'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Based on the results obtained, it appears that all sets except for the `Only Correlated` set have opted for using the Gini criterion. This criterion focuses on identifying the degree of impurity in the data, which is appropriate given the presence of numerous irrelevant features. On the other hand, the `Only Correlated` set has chosen to use the Entropy criterion, which is more concerned with the amount of information gained from a split. This is logical as this set only contains useful features.\n",
    "\n",
    "The remaining parameters are primarily influenced by the size of the data sets. The default set, which is larger, requires more search and hence more computational resources. In contrast, the smaller 70% data set does not require as much computational effort as it has fewer data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "Regarding the K-Nearest Neighbors model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50],\n",
    "    'p': [1, 2],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev']\n",
    "}\n",
    "\n",
    "grid_search_analysis(target_column, params, KNeighborsClassifier(), datasets, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are largely consistent across data sets (the `Polynomial`, `Scaled` and `Poly Scaled` datasets perform the worst).\n",
    "\n",
    "Looking at the selected parameters:\n",
    "- The algorithm is typically set to `auto` since it chooses the optimal algorithm. However, `kd_tree` may be the best choice here given the moderate data size (6000 rows).\n",
    "- The `uniform` weight is preferred for all data sets, indicating the data is **evenly distributed**, as expected.\n",
    "- The metric depends on the data distribution but is primarily `euclidean`, suggesting the values are **continuous**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "Regarding the Support Vector Machine model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [2, 3, 4, 5],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'coef0': [0, 1, 2, 3],\n",
    "    'shrinking': [True, False],\n",
    "    'tol': [1e-3, 1e-4, 1e-5],\n",
    "    'max_iter': [-1, 1000, 2000, 55000],\n",
    "    'decision_function_shape': ['ovr', 'ovo']\n",
    "}\n",
    "\n",
    "grid_search_analysis(target_column, params, SVC(), [sets['Scaled'], sets['Poly Scaled']], ['Scaled', 'Poly Scaled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Regarding the Stochastic Gradient Descent model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 2000, 3000],\n",
    "    'tol': [1e-3, 1e-4, 1e-5]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_analysis(target_column, params, SGDClassifier(), datasets, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}